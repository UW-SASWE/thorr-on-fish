{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data from [Columbia Basin Research](https://www.cbr.washington.edu/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## River Environment Data\n",
    "\n",
    "Outflow (kcfs), Spill (kcfs), Spill Percent (%), Inflow (kcfs), Temp (Scroll Case) (C), Temperature (C), Barometric Pressure (mmHg), Dissolved Gas (mmHg), Dissolved Gas Percent (%), Turbidity (ft), Elevation (ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../../../..\")\n",
    "data_dir = proj_dir / \"data/insitu/conditions\"\n",
    "\n",
    "stations_metadata_path = Path(proj_dir, \"data/insitu/metadata/stations.csv\")\n",
    "stations_attributes_path = Path(proj_dir, \"data/insitu/metadata/dictionaries/stations_attributes.csv\")\n",
    "\n",
    "stations_attributes = pd.read_csv(stations_attributes_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "dart_stations_metadata = pd.read_csv(Path(\"dart_metadata.csv\"))\n",
    "if not os.path.exists(stations_metadata_path):\n",
    "    stations_metadata = pd.DataFrame(columns=stations_attributes['Attribute_name'])\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "stations_metadata = pd.read_csv(stations_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the url for the request\n",
    "def format_url(proj: str, startDate: str, endDate: str):\n",
    "    \"\"\"Formats the url for the request for a particular project and date range within the same year\n",
    "    Args:\n",
    "        proj (str): abbreviated form of the project name\n",
    "        startDate (str): start date of the query [YYYY-MM-DD]\n",
    "        endDate (str): end date of the query [YYYY-MM-DD]\n",
    "    Returns:\n",
    "        url (str): formated url for the request\n",
    "    \"\"\"\n",
    "    # convert the dates to datetime objects\n",
    "    startDate = datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "    endDate = datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "\n",
    "    # get the year from the start date\n",
    "    year = startDate.year\n",
    "\n",
    "    # get the month and day from the start and end dates\n",
    "    startMonth = startDate.month\n",
    "    startDay = startDate.day\n",
    "    endMonth = endDate.month\n",
    "    endDay = endDate.day\n",
    "\n",
    "    # format the url\n",
    "    url = \"https://www.cbr.washington.edu/dart/cs/php/rpt/river_daily.php?sc=1&outputFormat=csv&year={}&proj={}&span=no&startdate={}%2F{}&enddate={}%2F{}\".format(\n",
    "        year, proj, startMonth, startDay, endMonth, endDay\n",
    "    )\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the url and convert it to a csv\n",
    "def get_data(proj: str, startDate: str, endDate: str, path: str):\n",
    "    \"\"\"Gets the data from the url and converts it to a csv\n",
    "    Args:\n",
    "        proj (str): abbreviated form of the project name\n",
    "        startDate (str): start date of the query [YYYY-MM-DD]\n",
    "        endDate (str): end date of the query [YYYY-MM-DD]\n",
    "        path (str): path to the directory where the csv file will be saved\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # capitalize the project name\n",
    "    proj = proj.upper()\n",
    "\n",
    "    # get start year and end year\n",
    "    startYear = datetime.strptime(startDate, \"%Y-%m-%d\").year\n",
    "    endYear = datetime.strptime(endDate, \"%Y-%m-%d\").year\n",
    "\n",
    "    first_data = True\n",
    "\n",
    "    # create a csv file for the data by adding all the data from each year\n",
    "    with open(\n",
    "        os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj)), \"w\", newline=\"\"\n",
    "    ) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        # for each year, take off the lines after the line that begins with 'Notes:'\n",
    "        for year in range(startYear, endYear + 1):\n",
    "            if year == startYear and year == endYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, startDate, endDate)\n",
    "            elif year == startYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, startDate, \"{}-12-31\".format(year))\n",
    "            elif year == endYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, \"{}-01-01\".format(year), endDate)\n",
    "            else:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, \"{}-01-01\".format(year), \"{}-12-31\".format(year))\n",
    "\n",
    "            # print(url)\n",
    "            # get the data from the url and convert it to csv format\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "            except requests.ConnectionError as e:\n",
    "                # sleep and try again\n",
    "                time.sleep(np.random.randint(20, 60))\n",
    "                response = requests.get(url)\n",
    "            # except requests.Timeout as e:\n",
    "            #     # stop the loop\n",
    "            #     break\n",
    "\n",
    "            \n",
    "            data = response.text.splitlines()\n",
    "            if (\n",
    "                data[0] == \"<!DOCTYPE html>\"\n",
    "                or data[0] == '<html lang=\"en\" class=\"no-js\">'\n",
    "            ):\n",
    "                pass\n",
    "            else:\n",
    "                for i in range(len(data)):\n",
    "                    if data[i].startswith(\"Notes:\"):\n",
    "                        data = data[:i]\n",
    "                        break\n",
    "                # write the data to the csv file but don't repeat the header row\n",
    "                # print(data[0])\n",
    "                if year == startYear or first_data:\n",
    "                    writer.writerows(csv.reader(data))\n",
    "                    first_data = False\n",
    "                else:\n",
    "                    writer.writerows(csv.reader(data[1:]))\n",
    "                # writer.writerows(csv.reader(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess the downloaded data\n",
    "def postprocess_data(proj: str, path: str, grand_id: str = None):\n",
    "    # if the data exists, read it in\n",
    "    if os.path.exists(os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj))):\n",
    "        df_existing = pd.read_csv(\n",
    "            os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj))\n",
    "        )\n",
    "        df_existing[\"date\"] = pd.to_datetime(df_existing[\"date\"])\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "\n",
    "    # read in the data\n",
    "    df = pd.read_csv(os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(\n",
    "        os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())), index=False\n",
    "    )\n",
    "    df = pd.read_csv(os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())))\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    new_df[\"outflow(m3/d)\"] = df[\"Outflow (kcfs)\"] * 0.0283168 * 86400 * 1000\n",
    "    new_df[\"inflow(m3/d)\"] = df[\"Inflow (kcfs)\"] * 0.0283168 * 86400 * 1000\n",
    "    new_df[\"spill(m3/d)\"] = df[\"Spill (kcfs)\"] * 0.0283168 * 86400 * 1000\n",
    "    new_df[\"avg_temp(C)\"] = df[\"Temperature (C)\"]\n",
    "    try:\n",
    "        new_df[\"wse(m)\"] = df[\"Elevation (ft)\"] * 0.3048\n",
    "    except:\n",
    "        try:\n",
    "            new_df[\"wse_tail(m)\"] = df[\"Tailwater Elevation (ft)\"] * 0.3048\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    df = None\n",
    "    # merge the data with existing data\n",
    "    if not df_existing.empty:\n",
    "        new_df = pd.concat([df_existing, new_df], ignore_index=True)\n",
    "        new_df.drop_duplicates(subset=[\"date\"], inplace=True)\n",
    "\n",
    "    # drop null columns\n",
    "    new_df = new_df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # save the data\n",
    "    new_df.to_csv(\n",
    "        os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj)), index=False\n",
    "    )\n",
    "\n",
    "    # print(new_df.columns)\n",
    "    return new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to break the dates into 5 year intervals\n",
    "def date_breaks(start: str, end: str, interval: int = 5):\n",
    "    \"\"\"Breaks the date range into 5 year intervals\n",
    "    Args:\n",
    "        start (str): start date of the date range [YYYY-MM-DD]\n",
    "        end (str): end date of the date range [YYYY-MM-DD]\n",
    "    Returns:\n",
    "        date_ranges (list): list of date ranges\n",
    "    \"\"\"\n",
    "    # convert the dates to datetime objects\n",
    "    start = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "\n",
    "    # get the years\n",
    "    startYear = start.year\n",
    "    endYear = end.year\n",
    "\n",
    "    # get the date ranges\n",
    "    date_ranges = []\n",
    "    for i in range(startYear, endYear, interval):\n",
    "        date_ranges.append(\n",
    "            (\n",
    "                \"{}-01-01\".format(i),\n",
    "                \"{}-12-31\".format(min(i + 4, endYear)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return date_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ALF data\n",
      "Finished processing ALF data\n",
      "Processing BON data\n",
      "Finished processing BON data\n",
      "Processing CCIW data\n",
      "Finished processing CCIW data\n",
      "Processing CIBW data\n",
      "Finished processing CIBW data\n",
      "Processing CHJ data\n",
      "Finished processing CHJ data\n",
      "Processing CHQW data\n",
      "Finished processing CHQW data\n",
      "Processing CWMW data\n",
      "Finished processing CWMW data\n",
      "Processing DWR data\n",
      "Finished processing DWR data\n",
      "Processing DWQI data\n",
      "Finished processing DWQI data\n",
      "Processing GCL data\n",
      "Finished processing GCL data\n",
      "Processing GCGW data\n",
      "Finished processing GCGW data\n",
      "Processing HGH data\n",
      "Finished processing HGH data\n",
      "Processing HGHM data\n",
      "Finished processing HGHM data\n",
      "Processing IHR data\n",
      "Finished processing IHR data\n",
      "Processing IDSW data\n",
      "Finished processing IDSW data\n",
      "Processing JDA data\n",
      "Finished processing JDA data\n",
      "Processing JHAW data\n",
      "Finished processing JHAW data\n",
      "Processing LEWI data\n",
      "Finished processing LEWI data\n",
      "Processing LIB data\n",
      "Finished processing LIB data\n",
      "Processing LGS data\n",
      "Finished processing LGS data\n",
      "Processing LGSW data\n",
      "Finished processing LGSW data\n",
      "Processing LWG data\n",
      "Finished processing LWG data\n",
      "Processing LGNW data\n",
      "Finished processing LGNW data\n",
      "Processing LMN data\n",
      "Finished processing LMN data\n",
      "Processing LMNW data\n",
      "Finished processing LMNW data\n",
      "Processing MCN data\n",
      "Finished processing MCN data\n",
      "Processing MCPW data\n",
      "Finished processing MCPW data\n",
      "Processing PAQW data\n",
      "Finished processing PAQW data\n",
      "Processing PRD data\n",
      "Finished processing PRD data\n",
      "Processing PRXW data\n",
      "Finished processing PRXW data\n",
      "Processing RIS data\n",
      "Finished processing RIS data\n",
      "Processing RIGW data\n",
      "Finished processing RIGW data\n",
      "Processing RRH data\n",
      "Finished processing RRH data\n",
      "Processing RRDW data\n",
      "Finished processing RRDW data\n",
      "Processing TDA data\n",
      "Finished processing TDA data\n",
      "Processing TDDO data\n",
      "Finished processing TDDO data\n",
      "Processing WAN data\n",
      "Finished processing WAN data\n",
      "Processing WANW data\n",
      "Finished processing WANW data\n",
      "Processing WRNO data\n",
      "Finished processing WRNO data\n",
      "Processing WEL data\n",
      "Finished processing WEL data\n",
      "Processing WELW data\n",
      "Finished processing WELW data\n",
      "Processing WFF data\n",
      "Finished processing WFF data\n"
     ]
    }
   ],
   "source": [
    "# get the data for each project\n",
    "proj = dart_stations_metadata[\n",
    "    \"Abbrev\"\n",
    "]  # list of projects https://www.cbr.washington.edu/dart/metadata/river\n",
    "# proj = [\"BON\", 'IHR', 'JDA']\n",
    "# grand_id = [297,  338, '338_tail', 299, '299_tail' ]\n",
    "startDate = \"1982-01-01\"\n",
    "# endDate = \"2024-09-18\"\n",
    "endDate = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# break the date range into 3 year intervals\n",
    "date_ranges = date_breaks(startDate, endDate, 1)\n",
    "\n",
    "# # specify the directory to save the data\n",
    "# data_dir = Path(\n",
    "#     \"/Users/gdarkwah/Library/CloudStorage/OneDrive-UW/01-Research/01-Hydrothermal History/Data/timeseries\"\n",
    "# )\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"raw/dart\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "\n",
    "# get the data for each project\n",
    "# for p, id in zip(proj, grand_id):\n",
    "\n",
    "# for date_range in date_ranges:\n",
    "for p in proj:\n",
    "    print(f\"Processing {p} data\")\n",
    "    get_data(p, startDate, endDate, data_dir)\n",
    "    parameters = postprocess_data(p, data_dir)\n",
    "\n",
    "    # if parameters is not None:\n",
    "\n",
    "    # update the metadata\n",
    "    station_ID = \"DART_\" + p.upper()\n",
    "    if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "        stations_metadata = pd.concat(\n",
    "            [\n",
    "                stations_metadata,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"station_ID\": [station_ID],\n",
    "                        \"id_at_source\": [p.upper()],\n",
    "                        \"available_data\": [\"{}\"],\n",
    "                        \"source_URL\": [\n",
    "                            '{\"url\" : []}'\n",
    "                        ],\n",
    "                        \"description\": [\n",
    "                            dart_stations_metadata[\n",
    "                                dart_stations_metadata[\"Abbrev\"] == p.upper()\n",
    "                            ][\"Project Name\"].values[0]\n",
    "                        ],\n",
    "                        \"latitude\": [\n",
    "                            dart_stations_metadata[\n",
    "                                dart_stations_metadata[\"Abbrev\"] == p.upper()\n",
    "                            ][\"Latitude\"].values[0]\n",
    "                        ],\n",
    "                        \"longitude\": [\n",
    "                            dart_stations_metadata[\n",
    "                                dart_stations_metadata[\"Abbrev\"] == p.upper()\n",
    "                            ][\"Longitude\"].values[0]\n",
    "                        ],\n",
    "                        \"site_params\": [\"{}\"],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # update source url\n",
    "    source_url = json.loads(\n",
    "        stations_metadata.loc[\n",
    "            stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "        ].values[0]\n",
    "    )\n",
    "\n",
    "    if \"https://www.cbr.washington.edu/dart/query/river_daily\" not in source_url[\"url\"]:\n",
    "        source_url[\"url\"].append(\"https://www.cbr.washington.edu/dart/query/river_daily\")\n",
    "        stations_metadata.loc[\n",
    "            stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "        ] = json.dumps(source_url)\n",
    "\n",
    "    # update the available data\n",
    "    availble_data = stations_metadata.loc[\n",
    "        stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "    ].values[0]\n",
    "    availble_data = json.loads(availble_data)\n",
    "\n",
    "    # check if there is \"conditions\"  in the available data\n",
    "    if \"conditions\" not in availble_data.keys():\n",
    "        availble_data[\"conditions\"] = []\n",
    "    # add the parameters to the available data\n",
    "    # print(parameters[1:])\n",
    "    for param in parameters[1:]:\n",
    "        if param not in availble_data[\"conditions\"]:\n",
    "            availble_data[\"conditions\"].append(param)\n",
    "\n",
    "    # update the metadata\n",
    "    stations_metadata.loc[\n",
    "        stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "    ] = json.dumps(availble_data)\n",
    "\n",
    "    print(f\"Finished processing {p} data\")\n",
    "    # # sleep for a random time between 30 to 60 seconds\n",
    "    # time.sleep(np.random.randint(30, 60))\n",
    "\n",
    "    # save the metadata\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last updated date and last updated by\n",
    "metadata_status = {\n",
    "    \"last_updated\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"update_message\": \"Updated the metadata to include the new DART stations\",\n",
    "    \"last_updated_by\": \"George Darkwah\",\n",
    "    \"last_updated_by_email\": \"gdarkwah@uw.edu\",\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "with open(Path(proj_dir, \"data/insitu/metadata/metadata_status.csv\"), \"w\") as f:\n",
    "    json.dump(metadata_status, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
