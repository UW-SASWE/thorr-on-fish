{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../../../..\")\n",
    "stations_metadata_path = Path(proj_dir, \"data/insitu/metadata/stations.csv\")\n",
    "stations_attributes_path = Path(proj_dir, \"data/insitu/metadata/dictionaries/stations_attributes.csv\")\n",
    "\n",
    "stations_attributes = pd.read_csv(stations_attributes_path)\n",
    "\n",
    "conditions_data = pd.read_csv(Path(proj_dir, \"data/insitu/metadata/dictionaries/conditions_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attribute_name   Unit                                        Description\n",
      "0    min_temp(C)  deg C           Observed daily minimum water temperature\n",
      "1    max_temp(C)  deg C           Observed daily maximum water temperature\n",
      "2    avg_temp(C)  deg C           Observed daily average water temperature\n",
      "3   inflow(m3/d)   m3/d  If location is a dam, this refers to the upstr...\n",
      "4  outflow(m3/d)   m3/d                       Discharge from dam or reach.\n",
      "5    spill(m3/d)   m3/d            Discharge through the spillway of a dam\n",
      "6         wse(m)      m  water surface elevation at the reach or upstre...\n",
      "7    wse_tail(m)      m            tailwater surface elevation at the dam.\n",
      "8    storage(m3)     m3            Volume of water stored in the reservoir\n"
     ]
    }
   ],
   "source": [
    "print(conditions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "if not os.path.exists(stations_metadata_path):\n",
    "    stations_metadata = pd.DataFrame(columns=stations_attributes['Attribute_name'])\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "stations_metadata = pd.read_csv(stations_metadata_path)\n",
    "\n",
    "usbr_stations_metadata = json.load(Path('stations.json').open(\"r\"))\n",
    "\n",
    "pcode_keys = usbr_stations_metadata[\"pcode_keys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format the url\n",
    "def format_url(station_name: str, pcodes: list, start: datetime, end: datetime):\n",
    "    \"\"\"Formats the url for the USBR PN data query.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "    Returns:\n",
    "        url (str): The formatted url.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://www.usbr.gov/pn-bin/daily.pl?station={station_name.lower()}&format=csv&year={start.year}&month={start.month}&day={start.day}&year={end.year}&month={end.month}&day={end.day}\"\n",
    "        + \"\".join([\"&pcode=\" + pcode.strip(\" \").lower() for pcode in pcodes])\n",
    "    )\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to download the data for a station\n",
    "def download_data(station_name: str, pcodes: list, start: datetime, end: datetime, path: str):\n",
    "    \"\"\"Downloads the data for a station.\n",
    "    Args:\n",
    "        station_name (str): The station name.\n",
    "        pcodes (list): The list of pcodes.\n",
    "        start (datetime): The start date.\n",
    "        end (datetime): The end date.\n",
    "        path (str): The path to save the data.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # format the url\n",
    "    url = format_url(station_name, pcodes, start, end)\n",
    "\n",
    "    # download the data\n",
    "    # r = requests.get(url)\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.ConnectionError as e:\n",
    "        # sleep and try again\n",
    "        time.sleep(np.random.randint(20, 60))\n",
    "        r = requests.get(url)\n",
    "    # except requests.Timeout as e:\n",
    "    #     # stop the loop\n",
    "    #     break\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        f.write(r.text)\n",
    "\n",
    "    # read the csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    # remove the header\n",
    "    data = data[1:]\n",
    "\n",
    "    # define the column names\n",
    "    column_names = [\"date\"] + pcodes\n",
    "\n",
    "    # write the data to a csv file\n",
    "    with open(os.path.join(path, 'raw/usbr', station_name + \".csv\"), \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(column_names)\n",
    "        writer.writerows(data)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from deg min sec to decimal degrees\n",
    "def dms2dd(degrees, minutes=0, seconds=0, direction=None):\n",
    "    dd = float(degrees) + float(minutes) / 60 + float(seconds) / (60 * 60)\n",
    "    if direction == \"S\" or direction == \"W\":\n",
    "        dd *= -1\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process the downloaded data\n",
    "def postprocess_data(\n",
    "    station_name: str,\n",
    "    path: str,\n",
    "    grand_id: str = None,\n",
    "    pcodes: list = None,\n",
    "    pcode_keys: dict = None,\n",
    "):\n",
    "    if not grand_id:\n",
    "        grand_id = station_name\n",
    "\n",
    "    # read in the data\n",
    "    # print(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(path, \"raw/usbr\", \"{}.csv\".format(station_name.upper()))\n",
    "    )\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[\"date\"] = df[\"date\"]\n",
    "\n",
    "    # convert the data to the correct units\n",
    "    for pcode in pcodes:\n",
    "        if pcode in pcode_keys.keys():\n",
    "            try:\n",
    "                pcode_keys[pcode][\"constant\"] = pcode_keys[pcode][\"constant\"]\n",
    "            except:\n",
    "                pcode_keys[pcode][\"constant\"] = None\n",
    "            \n",
    "            if pcode_keys[pcode][\"constant\"]:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = (\n",
    "                    df[pcode] * np.prod(pcode_keys[pcode][\"conversion_factors\"])\n",
    "                    + pcode_keys[pcode][\"constant\"]\n",
    "                )\n",
    "            else:\n",
    "                new_df[pcode_keys[pcode][\"column_name\"]] = df[pcode] * np.prod(\n",
    "                    pcode_keys[pcode][\"conversion_factors\"]\n",
    "                )\n",
    "\n",
    "    # save the data\n",
    "    new_df.to_csv(\n",
    "        # os.path.join(path, \"processed\", \"USBR_{}.csv\".format(grand_id)), index=False\n",
    "        os.path.join(path, \"processed\", \"USBR_{}.csv\".format(station_name)), index=False\n",
    "    )\n",
    "\n",
    "    return new_df.columns.tolist()\n",
    "    # print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the station names\n",
    "# station_names = [\"crpo\", 'prv', 'prvo', 'kee', 'cle', 'crao']\n",
    "station_names = pd.read_csv(\"pcodes.csv\", header=None)[0]\n",
    "# grand_ids = [None, 91, '91_forebay', 55, 58, None]\n",
    "\n",
    "# define the start and end dates\n",
    "start_date = datetime.strptime(\"1982-01-01\", \"%Y-%m-%d\")\n",
    "end_date = datetime.strptime(\"2024-02-28\", \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      AFCI\n",
       "1       AGA\n",
       "2      ALNO\n",
       "3      ALPY\n",
       "4      ALTO\n",
       "       ... \n",
       "285    WTXI\n",
       "286    WVCI\n",
       "287    YOKO\n",
       "288    YRPW\n",
       "289    YRWW\n",
       "Name: 0, Length: 290, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed data for AFCI\n",
      "processed data for AGA\n",
      "processed data for ALNO\n",
      "processed data for ALPY\n",
      "processed data for ALTO\n",
      "processed data for AMF\n",
      "processed data for AMFI\n",
      "processed data for ANCI\n",
      "processed data for AND\n",
      "processed data for ANDI\n",
      "processed data for ANTI\n",
      "processed data for ANTO\n",
      "processed data for ARK\n",
      "processed data for ARNO\n",
      "processed data for ASCI\n",
      "processed data for AUCI\n",
      "processed data for BASO\n",
      "processed data for BCAO\n",
      "processed data for BCMO\n",
      "processed data for BCSO\n",
      "processed data for BCTO\n",
      "processed data for BDDI\n",
      "processed data for BENO\n",
      "processed data for BEU\n",
      "processed data for BEUO\n",
      "processed data for BFCI\n",
      "processed data for BFKY\n",
      "processed data for BFTI\n",
      "processed data for BIGI\n",
      "processed data for BILI\n",
      "processed data for BIRO\n",
      "processed data for BJBO\n",
      "processed data for BKPI\n",
      "processed data for BMCI\n",
      "processed data for BOOI\n",
      "processed data for BPPI\n",
      "processed data for BRFI\n",
      "processed data for BSEI\n",
      "processed data for BTSI\n",
      "processed data for BUL\n",
      "processed data for BUM\n",
      "processed data for BURI\n",
      "processed data for CACO\n",
      "processed data for CBCI\n",
      "processed data for CCL\n",
      "processed data for CCPI\n",
      "processed data for CCR\n",
      "processed data for CECI\n",
      "processed data for CENO\n",
      "processed data for CFCI\n",
      "processed data for CFMM\n",
      "processed data for CHEI\n",
      "processed data for CIBW\n",
      "processed data for CLE\n",
      "processed data for CLS\n",
      "processed data for CMO\n",
      "processed data for CRA\n",
      "processed data for CRAO\n",
      "processed data for CRCI\n",
      "processed data for CRCO\n",
      "processed data for CRE\n",
      "processed data for CREO\n",
      "processed data for CRPO\n",
      "processed data for CRSO\n",
      "processed data for CSAO\n",
      "processed data for CSC\n",
      "processed data for CSCI\n",
      "processed data for CSRO\n",
      "processed data for CVPI\n",
      "processed data for CXCI\n",
      "processed data for CXMI\n",
      "processed data for DCMO\n",
      "processed data for DEBO\n",
      "processed data for DED\n",
      "processed data for DEDI\n",
      "processed data for DICO\n",
      "processed data for DLEO\n",
      "processed data for DNCI\n",
      "processed data for DRYI\n",
      "processed data for EASW\n",
      "processed data for EBCO\n",
      "processed data for EGCI\n",
      "processed data for EGSO\n",
      "processed data for ELCI\n",
      "processed data for EMI\n",
      "processed data for EMM\n",
      "processed data for ENTI\n",
      "processed data for EPTO\n",
      "processed data for ERCI\n",
      "processed data for ETSI\n",
      "processed data for FALI\n",
      "processed data for FARI\n",
      "processed data for FCEO\n",
      "processed data for FCFM\n",
      "processed data for FCSO\n",
      "processed data for FFCI\n",
      "processed data for FHPI\n",
      "processed data for FIS\n",
      "processed data for FLGY\n",
      "processed data for FOR\n",
      "processed data for FRCI\n",
      "processed data for FRMO\n",
      "processed data for FSHO\n",
      "processed data for FURO\n",
      "processed data for GCGW\n",
      "processed data for GILO\n",
      "processed data for GLI\n",
      "processed data for GREY\n",
      "processed data for GRS\n",
      "processed data for GSPO\n",
      "processed data for GSTO\n",
      "processed data for GWCI\n",
      "processed data for HARI\n",
      "processed data for HAY\n",
      "processed data for HEII\n",
      "processed data for HEN\n",
      "processed data for HENI\n",
      "processed data for HFAI\n",
      "processed data for HGH\n",
      "processed data for HGHM\n",
      "processed data for HPCO\n",
      "processed data for HPD\n",
      "processed data for HRSI\n",
      "processed data for HYA\n",
      "processed data for IDCI\n",
      "processed data for INCI\n",
      "processed data for ISCI\n",
      "processed data for ISL\n",
      "processed data for ISLI\n",
      "processed data for JCK\n",
      "processed data for JKSY\n",
      "processed data for JRPI\n",
      "processed data for KAC\n",
      "processed data for KEE\n",
      "processed data for KIOW\n",
      "processed data for KTCW\n",
      "processed data for LABI\n",
      "processed data for LACI\n",
      "processed data for LAPO\n",
      "processed data for LBCO\n",
      "processed data for LBEO\n",
      "processed data for LEFO\n",
      "processed data for LNRW\n",
      "processed data for LORI\n",
      "processed data for LOW\n",
      "processed data for LPPI\n",
      "processed data for LUC\n",
      "processed data for LWOI\n",
      "processed data for MABO\n",
      "processed data for MADO\n",
      "processed data for MALO\n",
      "processed data for MAN\n",
      "processed data for MAXO\n",
      "processed data for MCK\n",
      "processed data for MCKO\n",
      "processed data for MFDO\n",
      "processed data for MIII\n",
      "processed data for MIL\n",
      "processed data for MILI\n",
      "processed data for MIN\n",
      "processed data for MINI\n",
      "processed data for MLCI\n",
      "processed data for MLCO\n",
      "processed data for MORI\n",
      "processed data for MPCI\n",
      "processed data for MRYI\n",
      "processed data for MXCI\n",
      "processed data for MYKO\n",
      "processed data for NACW\n",
      "processed data for NCAO\n",
      "processed data for NFLO\n",
      "processed data for NLCI\n",
      "processed data for NMCO\n",
      "processed data for NPAO\n",
      "processed data for NPDO\n",
      "processed data for OCH\n",
      "processed data for OCHO\n",
      "processed data for OCRO\n",
      "processed data for OSCI\n",
      "processed data for OWY\n",
      "processed data for PAL\n",
      "processed data for PALI\n",
      "processed data for PARI\n",
      "processed data for PARW\n",
      "processed data for PAY\n",
      "processed data for PAYI\n",
      "processed data for PCKY\n",
      "processed data for PDTO\n",
      "processed data for PECI\n",
      "processed data for PHL\n",
      "processed data for PLCI\n",
      "processed data for PLEI\n",
      "processed data for POCI\n",
      "processed data for PRHO\n",
      "processed data for PRKI\n",
      "processed data for PRLI\n",
      "processed data for PRPI\n",
      "processed data for PRV\n",
      "processed data for PRVO\n",
      "processed data for PWDO\n",
      "processed data for RBDW\n",
      "processed data for RDWI\n",
      "processed data for RECI\n",
      "processed data for REXI\n",
      "processed data for RGCI\n",
      "processed data for RICI\n",
      "processed data for RIM\n",
      "processed data for RIR\n",
      "processed data for RJPI\n",
      "processed data for ROMO\n",
      "processed data for ROZW\n",
      "processed data for RRPI\n",
      "processed data for RSA\n",
      "processed data for RSCW\n",
      "processed data for RSDI\n",
      "processed data for RXRI\n",
      "processed data for SALY\n",
      "processed data for SBCO\n",
      "processed data for SBEO\n",
      "processed data for SCLO\n",
      "processed data for SCO\n",
      "processed data for SCOO\n",
      "processed data for SDCO\n",
      "processed data for SFLO\n",
      "processed data for SHYI\n",
      "processed data for SLBO\n",
      "processed data for SNAI\n",
      "processed data for SNCW\n",
      "processed data for SNDI\n",
      "processed data for SOL\n",
      "processed data for SPBO\n",
      "processed data for SPPI\n",
      "processed data for SQSO\n",
      "processed data for SUCI\n",
      "processed data for SWCO\n",
      "processed data for TCNI\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m pcode_keys \u001b[38;5;241m=\u001b[39m stations_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpcode_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# download the data\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_name\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# postprocess the data\u001b[39;00m\n\u001b[1;32m     25\u001b[0m station_conditions \u001b[38;5;241m=\u001b[39m postprocess_data(\n\u001b[1;32m     26\u001b[0m     station_name\u001b[38;5;241m.\u001b[39mupper(), data_dir, pcodes\u001b[38;5;241m=\u001b[39mpcodes, pcode_keys\u001b[38;5;241m=\u001b[39mpcode_keys\n\u001b[1;32m     27\u001b[0m )\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mdownload_data\u001b[0;34m(station_name, pcodes, start, end, path)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# download the data\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# r = requests.get(url)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# sleep and try again\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m60\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hydrothermal-history/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read the stations json file\n",
    "with open(\"stations.json\", \"r\") as f:\n",
    "    stations_dict = json.load(f)\n",
    "\n",
    "# specify the download folder and make it the current working directory\n",
    "data_dir = proj_dir / \"data/insitu/conditions\"\n",
    "\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "os.makedirs(os.path.join(data_dir, \"raw/usbr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)\n",
    "\n",
    "# download the data for each station\n",
    "# for station_name, id in zip(station_names, grand_ids):\n",
    "for station_name in station_names:\n",
    "    # if pcodes exist for the station\n",
    "    if \"pcodes\" in stations_dict[station_name.upper()]:\n",
    "        # define the pcodes and pcode keys\n",
    "        pcodes = stations_dict[station_name.upper()][\"pcodes\"]\n",
    "        pcode_keys = stations_dict[\"pcode_keys\"]\n",
    "\n",
    "        # download the data\n",
    "        download_data(station_name.upper(), pcodes, start_date, end_date, data_dir)\n",
    "        # postprocess the data\n",
    "        station_conditions = postprocess_data(\n",
    "            station_name.upper(), data_dir, pcodes=pcodes, pcode_keys=pcode_keys\n",
    "        )\n",
    "\n",
    "        # print(conditions_data)\n",
    "\n",
    "        # update the metadata\n",
    "        station_ID = f\"USBR_{station_name}\"\n",
    "\n",
    "        # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "        #     stations_metadata = pd.concat(\n",
    "        #         [\n",
    "        #             stations_metadata,\n",
    "        #             pd.DataFrame(\n",
    "        #                 {\n",
    "        #                     \"station_ID\": [station_ID],\n",
    "        #                     \"id_at_source\": [station_name.upper()],\n",
    "        #                     \"available_data\": [\"{}\"],\n",
    "        #                     \"source_URL\": [\n",
    "        #                         f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "        #                     ],\n",
    "        #                     \"description\": [\n",
    "        #                         usbr_stations_metadata[station_name][\"description\"]\n",
    "        #                     ],\n",
    "        #                     \"latitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                             direction=\"N\",\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"longitude\": [\n",
    "        #                         dms2dd(\n",
    "        #                             direction=\"W\",\n",
    "        #                             *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "        #                             .strip(\"-\")\n",
    "        #                             .split(\"-\"),\n",
    "        #                         )\n",
    "        #                     ],\n",
    "        #                     \"site_params\": [\"{}\"],\n",
    "        #                 }\n",
    "        #             ),\n",
    "        #         ],\n",
    "        #         ignore_index=True,\n",
    "        #     )\n",
    "\n",
    "        # # update the available data\n",
    "        # availble_data = stations_metadata.loc[\n",
    "        #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "        # ].values[0]\n",
    "        # availble_data = json.loads(availble_data)\n",
    "\n",
    "        # add the parameters to the available data\n",
    "        # print(parameters[1:])\n",
    "        \n",
    "        for param in station_conditions:\n",
    "\n",
    "            if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                new_stations_metadata = pd.DataFrame(\n",
    "                    {\n",
    "                        \"station_ID\": [station_ID],\n",
    "                        \"id_at_source\": [station_name.upper()],\n",
    "                        \"available_data\": [\"{}\"],\n",
    "                        \"source_URL\": ['{\"url\" : []}'],\n",
    "                        \"description\": [\n",
    "                            usbr_stations_metadata[station_name][\"description\"]\n",
    "                        ],\n",
    "                        \"latitude\": [\n",
    "                            dms2dd(\n",
    "                                *usbr_stations_metadata[station_name][\"latitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                                direction=\"N\",\n",
    "                            )\n",
    "                        ],\n",
    "                        \"longitude\": [\n",
    "                            dms2dd(\n",
    "                                direction=\"W\",\n",
    "                                *usbr_stations_metadata[station_name][\"longitude\"]\n",
    "                                .strip(\"-\")\n",
    "                                .split(\"-\"),\n",
    "                            )\n",
    "                        ],\n",
    "                        \"site_params\": [\"{}\"],\n",
    "                    }\n",
    "                )\n",
    "                availble_data = json.loads(\n",
    "                    new_stations_metadata[\"available_data\"].values[0]\n",
    "                )\n",
    "                stations_metadata = pd.concat(\n",
    "                    [\n",
    "                        stations_metadata,\n",
    "                        new_stations_metadata,\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                availble_data = stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ].values[0]\n",
    "                availble_data = json.loads(availble_data)\n",
    "\n",
    "            # update source url\n",
    "            source_url = json.loads(\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ].values[0]\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                not in source_url[\"url\"]\n",
    "            ):\n",
    "                source_url[\"url\"].append(\n",
    "                    f\"https://www.usbr.gov/pn-bin/inventory.pl?site={station_name.upper()}&ui=true&interval=daily\"\n",
    "                )\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "                ] = json.dumps(source_url)\n",
    "\n",
    "            # check if there is \"conditions\"  in the available data\n",
    "            if \"conditions\" not in availble_data.values():\n",
    "                availble_data[\"conditions\"] = []\n",
    "\n",
    "            # # update the available data\n",
    "            # availble_data = stations_metadata.loc[\n",
    "            #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "            # ].values[0]\n",
    "            # availble_data = json.loads(availble_data)\n",
    "\n",
    "            # print((param not in availble_data[\"conditions\"]) and (param in conditions_data['Attribute_name'].to_list()))\n",
    "            if (param not in availble_data[\"conditions\"]) and (\n",
    "                param in conditions_data[\"Attribute_name\"].to_list()\n",
    "            ):\n",
    "\n",
    "                availble_data[\"conditions\"].append(param)\n",
    "\n",
    "                # if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "                #     new_stations_metadata[\"available_data\"] = json.dumps(availble_data)\n",
    "                #     stations_metadata = pd.concat(\n",
    "                #         [\n",
    "                #             stations_metadata,\n",
    "                #             new_stations_metadata,\n",
    "                #         ],\n",
    "                #         ignore_index=True,\n",
    "                #     )\n",
    "                # else:\n",
    "                #     # update the metadata\n",
    "                #     stations_metadata.loc[\n",
    "                #         stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                #     ] = json.dumps(availble_data)\n",
    "\n",
    "                # update the metadata\n",
    "                stations_metadata.loc[\n",
    "                    stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                ] = json.dumps(availble_data)\n",
    "\n",
    "                # # update the metadata\n",
    "                # stations_metadata.loc[\n",
    "                #     stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "                # ] = json.dumps(availble_data)\n",
    "\n",
    "                # save the metadata\n",
    "                stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "    print(\"processed data for {}\".format(station_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last updated date and last updated by\n",
    "metadata_status = {\n",
    "    \"last_updated\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"update_message\": \"Updated the metadata for USBR stations\",\n",
    "    \"last_updated_by\": \"George Darkwah\",\n",
    "    \"last_updated_by_email\": \"gdarkwah@uw.edu\",\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "with open(Path(proj_dir, \"data/insitu/metadata/metadata_status.csv\"), \"w\") as f:\n",
    "    json.dump(metadata_status, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
