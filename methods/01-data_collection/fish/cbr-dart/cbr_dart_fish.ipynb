{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = Path(\"../../../../\")\n",
    "data_dir = proj_dir / \"data/insitu/fish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_count_sites = pd.read_csv(\"site_table.csv\")\n",
    "stations_metadata_path = proj_dir / \"data/insitu/metadata/stations.csv\"\n",
    "stations_metadata = pd.read_csv(stations_metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(lat_lon_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_locations = [\n",
    "    x\n",
    "    for x in fish_count_sites[\"Abbrev\"]\n",
    "    if f\"DART_{x}\" not in stations_metadata[\"station_ID\"].values\n",
    "]\n",
    "lat_lon_dict = {\n",
    "    \"LYL\": {\"lat\": 45.7163626949042, \"lon\": -121.2595910431273},\n",
    "    \"PRO\": {\"lat\": 46.213277560318176, \"lon\": -119.77283276031208},\n",
    "    \"ROZ\": {\"lat\": 46.74919299784255, \"lon\": -120.46569546028712},\n",
    "    \"TUM\": {\"lat\": 47.61688013975446, \"lon\": -120.72316429020638},\n",
    "    \"ZOS\": {\"lat\": 48.933696826389344, \"lon\": -119.41963260251167},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard key per the THORR project\n",
    "data_key = {\n",
    "    \"Chin\": 'chinook', \"JChin\": 'chinook','Stlhd': 'steelhead','WStlhd': 'steelhead','Sock': 'sockeye','Coho': 'coho','JCoho': 'coho','Shad': 'shad','Lmpry': 'lamprey','BTrout': 'bull_trout','Chum': 'chum','Pink': 'pink','TempC': 'avg_temp(C)'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the url for the request\n",
    "def format_url(proj: str, startDate: str, endDate: str):\n",
    "    \"\"\"Formats the url for the request for a particular project and date range within the same year\n",
    "    Args:\n",
    "        proj (str): abbreviated form of the project name\n",
    "        startDate (str): start date of the query [YYYY-MM-DD]\n",
    "        endDate (str): end date of the query [YYYY-MM-DD]\n",
    "    Returns:\n",
    "        url (str): formated url for the request\n",
    "    \"\"\"\n",
    "    # convert the dates to datetime objects\n",
    "    startDate = datetime.strptime(startDate, \"%Y-%m-%d\")\n",
    "    endDate = datetime.strptime(endDate, \"%Y-%m-%d\")\n",
    "\n",
    "    # get the year from the start date\n",
    "    year = startDate.year\n",
    "\n",
    "    # get the month and day from the start and end dates\n",
    "    startMonth = startDate.month\n",
    "    startDay = startDate.day\n",
    "    endMonth = endDate.month\n",
    "    endDay = endDate.day\n",
    "\n",
    "    # format the url\n",
    "    url = f\"https://www.cbr.washington.edu/dart/cs/php/rpt/adult_daily.php?outputFormat=csv&year={year}&proj={proj}&span=no&startdate={startMonth}%2F{startDay}&enddate={endMonth}%2F{endDay}\"\n",
    "\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data from the url and convert it to a csv\n",
    "def get_data(proj: str, startDate: str, endDate: str, path: str):\n",
    "    \"\"\"Gets the data from the url and converts it to a csv\n",
    "    Args:\n",
    "        proj (str): abbreviated form of the project name\n",
    "        startDate (str): start date of the query [YYYY-MM-DD]\n",
    "        endDate (str): end date of the query [YYYY-MM-DD]\n",
    "        path (str): path to the directory where the csv file will be saved\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # capitalize the project name\n",
    "    proj = proj.upper()\n",
    "\n",
    "    # get start year and end year\n",
    "    startYear = datetime.strptime(startDate, \"%Y-%m-%d\").year\n",
    "    endYear = datetime.strptime(endDate, \"%Y-%m-%d\").year\n",
    "\n",
    "    first_data = True\n",
    "\n",
    "    # create a csv file for the data by adding all the data from each year\n",
    "    with open(\n",
    "        os.path.join(path, 'raw/dart/' \"DART_{}.csv\".format(proj)), \"w\", newline=\"\"\n",
    "    ) as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=\",\")\n",
    "        # for each year, take off the lines after the line that begins with 'Notes:'\n",
    "        for year in range(startYear, endYear + 1):\n",
    "            if year == startYear and year == endYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, startDate, endDate)\n",
    "            elif year == startYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, startDate, \"{}-12-31\".format(year))\n",
    "            elif year == endYear:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, \"{}-01-01\".format(year), endDate)\n",
    "            else:\n",
    "                # get the url for the request\n",
    "                url = format_url(proj, \"{}-01-01\".format(year), \"{}-12-31\".format(year))\n",
    "\n",
    "            # print(url)\n",
    "            # get the data from the url and convert it to csv format\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "            except requests.ConnectionError as e:\n",
    "                # sleep and try again\n",
    "                time.sleep(np.random.randint(20, 60))\n",
    "                response = requests.get(url)\n",
    "\n",
    "                \n",
    "            data = response.text.splitlines()\n",
    "            if (\n",
    "                data[0] == \"<!DOCTYPE html>\"\n",
    "                or data[0] == '<html lang=\"en\" class=\"no-js\">'\n",
    "            ):\n",
    "                pass\n",
    "            else:\n",
    "                for i in range(len(data)):\n",
    "                    if data[i].startswith(\"Notes:\"):\n",
    "                        data = data[:i]\n",
    "                        break\n",
    "                # write the data to the csv file but don't repeat the header row\n",
    "                # print(data[0])\n",
    "                if year == startYear or first_data:\n",
    "                    writer.writerows(csv.reader(data))\n",
    "                    first_data = False\n",
    "                else:\n",
    "                    writer.writerows(csv.reader(data[1:]))\n",
    "                # writer.writerows(csv.reader(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# postprocess the downloaded data\n",
    "def postprocess_data(proj: str, path: str, conditions_path = None):\n",
    "    # if the data exists, read it in\n",
    "    if os.path.exists(os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj))):\n",
    "        df_existing = pd.read_csv(\n",
    "            os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj))\n",
    "        )\n",
    "        df_existing[\"date\"] = pd.to_datetime(df_existing[\"date\"])\n",
    "    else:\n",
    "        df_existing = pd.DataFrame()\n",
    "\n",
    "    # read in the data\n",
    "    df = pd.read_csv(os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())))\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_csv(\n",
    "        os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())), index=False\n",
    "    )\n",
    "    df = pd.read_csv(os.path.join(path, \"raw/dart\", \"DART_{}.csv\".format(proj.upper())))\n",
    "\n",
    "    new_df = pd.DataFrame(columns=[\"date\"]+list(set(data_key.values())))\n",
    "    new_df[\"date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    \n",
    "    new_df['chinook'] = (df['Chin'].fillna(0) + df['JChin'].fillna(0)).replace(0, np.nan)\n",
    "    new_df['steelhead'] = (df['Stlhd'].fillna(0) + df['WStlhd'].fillna(0)).replace(0, np.nan)\n",
    "    new_df['sockeye'] = df['Sock'].replace(0, np.nan)\n",
    "    new_df['coho'] = (df['Coho'].fillna(0) + df['JCoho'].fillna(0)).replace(0, np.nan)\n",
    "    new_df['shad'] = df['Shad'].replace(0, np.nan)\n",
    "    new_df['lamprey'] = df['Lmpry'].replace(0, np.nan)\n",
    "    new_df['bull_trout'] = df['BTrout'].replace(0, np.nan)\n",
    "    new_df['chum'] = df['Chum'].replace(0, np.nan)\n",
    "    new_df['pink'] = df['Pink'].replace(0, np.nan)\n",
    "    new_df['avg_temp(C)'] = df['TempC']\n",
    "\n",
    "\n",
    "    df = None\n",
    "    # merge the data with existing data\n",
    "    if not df_existing.empty:\n",
    "        new_df = pd.concat([df_existing, new_df], ignore_index=True)\n",
    "        new_df.drop_duplicates(subset=[\"date\"], inplace=True)\n",
    "\n",
    "    # drop null columns\n",
    "    new_df = new_df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    # save the data\n",
    "    new_df.to_csv(\n",
    "        os.path.join(path, \"processed\", \"DART_{}.csv\".format(proj)), index=False\n",
    "    )\n",
    "\n",
    "    if 'avg_temp(C)' in new_df.columns:\n",
    "        # load conditions dataset if it exists\n",
    "        if os.path.exists(os.path.join(conditions_path ,\"processed\", f\"DART_{proj}.csv\")):\n",
    "            existing_conditions = pd.read_csv(os.path.join(conditions_path ,\"processed\", f\"DART_{proj}.csv\"))\n",
    "            existing_conditions[\"date\"] = pd.to_datetime(existing_conditions[\"date\"])\n",
    "        else:\n",
    "            existing_conditions = pd.DataFrame()\n",
    "        \n",
    "        # merge the data with existing data\n",
    "        if not existing_conditions.empty:\n",
    "            existing_conditions = existing_conditions.merge(new_df[['date', 'avg_temp(C)']], on=\"date\", how=\"outer\")\n",
    "        else:\n",
    "            existing_conditions = new_df[['date', 'avg_temp(C)']]\n",
    "\n",
    "        if 'avg_temp(C)_x' in existing_conditions.columns:\n",
    "            existing_conditions['avg_temp(C)'] =existing_conditions['avg_temp(C)_x'].where(existing_conditions['avg_temp(C)_x'].notnull(), existing_conditions['avg_temp(C)_y'])\n",
    "            existing_conditions.drop(columns=['avg_temp(C)_x', 'avg_temp(C)_y'], inplace=True)\n",
    "\n",
    "        # save the data\n",
    "        # print(path, \"/../conditons/processed\", f\"DART{proj}.csv\")\n",
    "        existing_conditions.to_csv(\n",
    "            Path(conditions_path ,\"processed\", f\"DART_{proj}.csv\"), index=False\n",
    "        )\n",
    "\n",
    "    # print(new_df.columns)\n",
    "    return new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = \"1939-01-01\"\n",
    "# endDate = \"2024-09-18\"\n",
    "endDate = datetime.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"raw/dart\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, \"processed\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing BON data\n",
      "Finished processing IHR data\n",
      "Finished processing JDA data\n",
      "Finished processing LGS data\n",
      "Finished processing LMN data\n",
      "Finished processing LWG data\n",
      "Finished processing LYL data\n",
      "Finished processing MCN data\n",
      "Finished processing PRD data\n",
      "Finished processing PRO data\n",
      "Finished processing RIS data\n",
      "Finished processing ROZ data\n",
      "Finished processing RRH data\n",
      "Finished processing TDA data\n",
      "Finished processing TUM data\n",
      "Finished processing WAN data\n",
      "Finished processing WEL data\n",
      "Finished processing WFF data\n",
      "Finished processing ZOS data\n"
     ]
    }
   ],
   "source": [
    "for p in fish_count_sites[\"Abbrev\"]:\n",
    "# for p in list(lat_lon_dict.keys()):\n",
    "# for p in ['ZOS',]:\n",
    "    get_data(p, startDate, endDate, data_dir)\n",
    "    parameters = postprocess_data(\n",
    "        p, data_dir, conditions_path=Path(proj_dir, \"data/insitu/conditions\")\n",
    "    )\n",
    "\n",
    "    # update metadata and existing conditions file if temperature is included\n",
    "    station_ID = \"DART_\" + p.upper()\n",
    "\n",
    "    if station_ID not in stations_metadata[\"station_ID\"].values:\n",
    "        lat = lat_lon_dict[p][\"lat\"]\n",
    "        lon = lat_lon_dict[p][\"lon\"]\n",
    "\n",
    "        project = fish_count_sites[fish_count_sites[\"Abbrev\"] == p.upper()][\n",
    "            \"Project/Dam\"\n",
    "        ].values[0]\n",
    "        river = fish_count_sites[fish_count_sites[\"Abbrev\"] == p.upper()][\n",
    "            \"River\"\n",
    "        ].values[0]\n",
    "        description = f\"{project} - {river}\"\n",
    "\n",
    "        stations_metadata = pd.concat(\n",
    "            [\n",
    "                stations_metadata,\n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        \"station_ID\": [station_ID],\n",
    "                        \"id_at_source\": [p.upper()],\n",
    "                        \"available_data\": [\"{}\"],\n",
    "                        \"source_URL\": [\n",
    "                            '{\"url\" : []}'\n",
    "                        ],\n",
    "                        \"description\": [description],\n",
    "                        # \"latitude\": [\n",
    "                        #     dart_stations_metadata[\n",
    "                        #         dart_stations_metadata[\"Abbrev\"] == p.upper()\n",
    "                        #     ][\"Longitude\"].values[0]\n",
    "                        # ],\n",
    "                        # \"longitude\": [\n",
    "                        #     dart_stations_metadata[\n",
    "                        #         dart_stations_metadata[\"Abbrev\"] == p.upper()\n",
    "                        #     ][\"Latitude\"].values[0]\n",
    "                        # ],\n",
    "                        \"latitude\": [lat],\n",
    "                        \"longitude\": [lon],\n",
    "                        \"site_params\": [\"{}\"],\n",
    "                    }\n",
    "                ),\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # update source url\n",
    "    source_url = json.loads(\n",
    "        stations_metadata.loc[\n",
    "            stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "        ].values[0]\n",
    "    )\n",
    "\n",
    "    if \"https://www.cbr.washington.edu/dart/query/adult_daily\" not in source_url[\"url\"]:\n",
    "        source_url[\"url\"].append(\"https://www.cbr.washington.edu/dart/query/adult_daily\")\n",
    "        stations_metadata.loc[\n",
    "            stations_metadata[\"station_ID\"] == station_ID, \"source_URL\"\n",
    "        ] = json.dumps(source_url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # update the available data\n",
    "    availble_data = stations_metadata.loc[\n",
    "        stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "    ].values[0]\n",
    "    availble_data = json.loads(availble_data)\n",
    "\n",
    "    # check if there is \"conditions\"  in the available data\n",
    "    if \"fish\" not in availble_data.keys():\n",
    "        availble_data[\"fish\"] = []\n",
    "    # add the parameters to the available data\n",
    "    # print(parameters[1:])\n",
    "    for param in parameters[1:]:\n",
    "        if (param not in availble_data[\"fish\"]) and (param != \"avg_temp(C)\"):\n",
    "            availble_data[\"fish\"].append(param)\n",
    "        elif param == \"avg_temp(C)\" and \"conditions\" not in availble_data.keys():\n",
    "            availble_data[\"conditions\"] = [\"avg_temp(C)\"]\n",
    "        elif param == \"avg_temp(C)\" and \"conditions\" in availble_data.keys():\n",
    "            if \"avg_temp(C)\" not in availble_data[\"conditions\"]:\n",
    "                availble_data[\"conditions\"].append(\"avg_temp(C)\")\n",
    "\n",
    "    # update the metadata\n",
    "    stations_metadata.loc[\n",
    "        stations_metadata[\"station_ID\"] == station_ID, \"available_data\"\n",
    "    ] = json.dumps(availble_data)\n",
    "\n",
    "    # save the metadata\n",
    "    stations_metadata.to_csv(stations_metadata_path, index=False)\n",
    "\n",
    "    print(f\"Finished processing {p} data\")\n",
    "    # # sleep for a random time between 30 to 60 seconds\n",
    "    # time.sleep(np.random.randint(30, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add last updated date and last updated by\n",
    "metadata_status = {\n",
    "    \"last_updated\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"update_message\": \"Updated the metadata to include the new DART stations and fish data\",\n",
    "    \"last_updated_by\": \"George Darkwah\",\n",
    "    \"last_updated_by_email\": \"gdarkwah@uw.edu\",\n",
    "}\n",
    "\n",
    "# save metadata\n",
    "with open(Path(proj_dir, \"Data/insitu/metadata/metadata_status.csv\"), \"w\") as f:\n",
    "    json.dump(metadata_status, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrothermal-history",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
